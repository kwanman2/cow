=====
To use Stampede do:
(Note, other sytems are very similar to one kind of system listed in makeallmovie.sh.  E.g., Kraken uses qsub like Pleides).  So you can modify the conditionals in makemovieall.sh to choose an existing system *number* but using a different check (i.e. iskraken modified for Pleides).  Then, in makemovie.sh, one still modifies everything like one should for that fake system (but chosen system number).
=====

1) Get XSEDE user account and give username to Jon.

2) Get access from Jon.

3) Login with user@stampede.tacc.utexas.edu.

4) See if ~/.ssh/ keys exist.  If already exist, copy public key (id_rsa.pub or similar .pub file) to harm.unfuddle.com's part where your user has a list of keys.  If not exist, create using ssh-keygen (or google about it as maybe what that generates needs to be copied to id_rsa and id_rsa.pub).

5) git clone ... for harm and python stuff. Change to koralinsert branch and compile harm stuff as usual.  Copy over python directory to ~/py/ as usual -- follow python directions.

6) in harmgit/environmentfiles/stampede/ there are normal *and* hidden files.  cp all those files to ~/ on stampede.

7) Exit and re-login to stampede so .bash files are used that you just copied.

8) On stampede in ~/ create directory for your run's code by doing (e.g.) cp -a harmgit rad0.3 or rad1 or rad5.  Ensure you've compiled as a check that compiling works.

9) Edit the init.h N1,N2,N3 for RADDONUT section as required for per-core values.  Compile harm as usual.  Ignore "icc: error #10103: can't fork process"

10) in ~/rad0.3/batches/ there is batch.slurm.kraken.radtma0.8a that you can use as a template batch file for slurm (like qsub).  Copy this to the ~/rad0.3/ (or other like) directory.

Edit this file by changing 4096 to total number of NCPUX1*NCPUX2*NCPUX3 you will use.  Change 48:00:00 to (say) 00:01:00 for 1 hour test run.  Change email to yours.  Change "radtma0.8" everywhere to "rad0.3" everywhere.

Edit NCPUX1,NCPUX2,NCPUX3 to what you want to use.  These are number of cores in each direction.  Total resolution is (NCPUX1*N1)x(NCPUX2*N2)x(NCPUX3*N3) .  You need N to be 1 if that dimension is not present.  Minimum N is otherwise 4 and good choice is minimum of 8.

set NTOT to be the NCPUX1*NCPUX2*NCPUX3.  At first use 16 total cores in the same 4x4x1 configuration as on BH.

Edit everywhere tg802609 to your username.  This includes changing the "DIRFILE" to point to your correct home directory and code directory where "grmhd" is located and was compiled.

11) In more detail on choosing cores and resolution:


You always control the per-core res (N123) and the number of cores in each direction (ncpux123) that you choose to get whatever total res you want.  Although, often the total res is kinda chosen for you based upon if your testing or doing production runs and the need to get runs done quickly for testing.

Let us all use 128x64x16 for the total res for now.

With the minimum per-core resolution of N123=8x8x8 one has up to ncpux123 = 16x8x2 (i.e. 256 cores total).  That will take a while to get going in the queue and I don't recommend that until the density is tuned better.

So instead I recommend using something like 64 cores, so the job gets going quickly and you only wait for the first 250 field line files (hopefully within no restart needed).  So use ncpux123=8x8x1 (i.e. 64 cores as desired) with (based upon total resolution needed) N123=16x8x16 .

Try this with 1 hour first, and if it works, then try 24 hours.  Or estimate the hours needed to get to 250 field line files.  If it ends up you need to restart the run to get to 250 field line files, then I'll provide docs on how to manually restart by editing the script (rather than automated as used when doing production runs and doing many restarts).

12) Run the batch by doing on the command line:

sbatch batch.slurm.kraken.radtma0.8a

13) If need to restart the run manually, then do the following:

a) cd ~/rada0.3/

b) cp batches/stampederestartsustain_rad1.sh .

c) cp batch.slurm.kraken.radtma0.8a batch.slurm.kraken.radtma0.8b

d) edit the batch.slurm.kraken.radtma0.8b file and change "rad0.3a" to "rad0.3b" everywhere.  Change firstrun to 0 .

e) sbatch batch.slurm.kraken.radtma0.8b

This will use stampederestartsustain_rad1.sh to copy over the necessary files from $SCRATCH/rad0.3a/ to $SCRATCH/rad0.3b/ .

14) Let run until (say) 250 fieldline files are made.  For 128x64x16 this will take about 48 hours, so choose 48 hours in the batch script.







=====
Running python scripts on stampede:
=====

It's mostly following general tutorial for python stuff already at
http://physics-179.umd.edu/doxygenresults_verypruned/html/general__plotting__guide_8txt.html
, but there are some stampede-specific things mentioned here that you need to consider or change in makeallmovie.sh and makemovie.sh .

1) Be on stampede

2) Ensure you have the latest pythontools repo stuff

cd ~/py/
git pull
# if it complains about overwriting files, move the mentioned file to a backup name.  Then compare to see any changes you made in case important.

#I should remind you to always change the email address to yours in makemovie.sh and makeallmovie.sh after you update pythontools.  The pseudotensor@gmail.com occurs in one place in each file near the top of the file.

3) Edit ~/py/scripts/makemovie.sh and search for "stampede" that will bring you to the stampede section where some changes are required.  Common things to change are:

a) "timetot" that is how long is needed to run any of makeavg, make1d, or makemovie.  If you run all 250 files at once, this will use 250 cores and needs about 1 hours I think.  If you ever get an email from the batch system that says the job timed-out during the makeavg, make1d, or makemovie stages, then you probably have to increase this time.

b) "numtasks" that is currently set as the total number of fieldline files.  As you later make more fieldline files, you will not want to use this same option because it will keep asking for the number of cores as the number of fieldline files. An upper limit on the number of cores is probably 512 as a reasonable option.  Then set numtasks=512 if you know that the number of fieldline files is above 512.  In that case, "timetot" should be increased proportionally by (#fieldline files)/(512) once you go beyond 512 fieldline files and fix numtasks to 512.

c) "numtaskspernode" that can be 16 and only needs to be 14 for high-res runs at resolution 256x128x64 or higher.

d) "timetotplot" that is 1 hour by default, which may be fine.  Check if it completes the "makeplot" stage or did not.  If you ever get timed-out from the batch system emails, you know you didn't give enough time.

4) Optionsal for now: Edit ~/py/__init__.py so that your run name used (say rad0.3) is present as a model list in (at least) def isradmodel(modelname): as another or conditional like or modelname=="rad0.3"  .  You can scan through the file and see how this "isradmodel" is used.  For more careful control over what happens with specific models, you can scan through for examples where "isradmodel" is referred to in the python script.  It's good to also add your model to the place where "fieldtype" and "truemodelname" and "gridtype" are set by adding your model as another conditional.  You can look in the script for modelname=="rad1" to see an example of how a specific model chooses those and other things.

5) Edit ~/py/__init__.py for stampede-specific things to optimize performance.  Uncomment the following lines:

#time.sleep(10.0+100.0*n) # on supercomputer's need to wait so file system gets up to date

#time.sleep(10.0*n) # on supercomputer's need to wait so file system gets up to date

6) By default, makeallmovie.sh will detect stampede and go into so-called "parallel=2" mode where it submits a batch job and runs an MPI wrapper that calls python for each core independently.  So it's an embarrassingly parallel job. To ensure the parallel==2 program will compile properly, go ahead and check by doing:

cd ~/py/scripts/
make
# you should get no warnings or errors and "makemoviec" should be newly created or overwrite old one.  This make happens during "makemovie.sh" but this is a check to ensure that will not have issues.  Sometimes TACC changes the MPI compiler or compilers get removed and so things have to be updated.

7) By default the batches create model"a" and further restarts should be "b" "c", etc.  I have a script that makes the final directory needed for python.  One does:

sh ~/py/scripts/createlinkssimple.sh rad0.3

This will take all the rad0.3a rad0.3b etc. and make a single directory rad0.3.

8) cp ~/py/scripts/makeallmovie.sh $SCRATCH ; cd $SCRATCH


9) Now edit makeallmovie.sh .

a) Do your normal edit of "dirruns=rad0.3" and "dircollect=rad0.3" .  export your moviename (test1 at first as below, and then, say, movie1) as described in the original python tutorial.

b) Try setting "numkeep" to 50 for a test.  Maybe call moviename "test1" or something.  After that test worked completely, *then* you can edit "numkeep" to be as large or larger than your number of fieldline files for a production python run.

c) Optional at first when testing: Edit the other things in makeallmovie.sh, such as adding a section to better control which files are kept by including a new section (among the other similar sections) to have:

       if [ "$thedir" == "rad0.3" ]
        then
            keepfilesstart=$(( 100 ))
           keepfilesend=$(( $numfiles ))
        fi

Or something similar.  This allows you to, for just calculations and not commonly done for movies, to focus resources on the range of files you want to time average, for example.

10) Run the first makeallmovie.sh line as in the tutorial (optional note: you can set numtasks to 20X less than the default you would use for make1d or makemovie, because the makeavg step is much faster and doesn't need so many cores, but then you need to change numtasks back to your default choice.  This can speed up getting through the queue).

bash ./makeallmovie.sh ${moviename} 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 # setup links, copy files, and makeavg step that makes avg2d_*.npy by submitting job

You have to wait for completion and creation of the avg2d.py file.  You can check $SCRATCH/rad0.3/test1/ for what is going on inside makemovielocal_radtest.out and makemovielocal_radtest.full.out  as well as 0ma.uk.out and 0ma.uk.err .

You can check if the job got started by doing "showq -u $USERNAME" and seeing if the job is in queue or running or not there yet (or maybe died already if you got an email of failure).

For a simple harm simulation with only 50 files, it should only take a few minutes on stampede to start (nominally) and a few minutes to finish creating the avg2d20_*.npy files.  After the job is done, the script makeallmovie.sh will continue and try to merge those _*.npy files into the single avg2d.npy file.  But this depends upon the script checking if the job got done, which is only done every 5 minutes.  This is why I removed the "1 1" in the above.  So now you should do the merge separately:

bash ./makeallmovie.sh ${moviename} 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 # makes merged avg2d.npy

This way you won't have to wait for small jobs to get done with the local merge step.  This should create after a few minutes the avg2d.npy file.


11) Only after the first line (or the split version above for small jobs) batch job is done and avg2d.npy is created do you run the normal 2nd line.  But again, let's split this up so the makeallmovie.sh doesn't wait for jobs to get done by itself -- for larger jobs it's ok to let makeallmovie.sh handle things, but not yet.  One does each of the below, but checks the results before moving onto the next line.

bash ./makeallmovie.sh ${moviename} 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # makes qty2_*.npy files, submits job.

bash ./makeallmovie.sh ${moviename} 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 # merges to get qty2.npy file, local job.

bash ./makeallmovie.sh ${moviename} 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 # makeplot step to make data*.txt and latex stuf in python.plot*, submits job.

bash ./makeallmovie.sh ${moviename} 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 # makemovie step, submits job.

bash ./makeallmovie.sh ${moviename} 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 # create mp4 movie, local job.

bash ./makeallmovie.sh ${moviename} 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 # makeavgplot, submits job.

For each case, you need to check that the number of cores you chose is reasonable when looking at the job using: "showq -u $USERNAME".  You also need to, for each case, before moving onto the next step, need to ensure it did what it should have done after the job completed according to "showq" or you got the email saying it completed  The meaning of completion is stated in # comment at end of line.

Once you get the hang of things, some things you can do simultaneously.  E.g., once avg2d.npy and qty2.npy are there, you can simultaneously do the "makeplot" "makemovie" and "makeavgplot" steps without waiting.  But wait for now to ensure you really follow things.

12) Also, you might skip the "create mp4 movie" line and just do:

cd $SCRATCH/rad0.3/test1/
cp ~/py/scripts/makelinkimagenew2.sh .
# edit makelinkimagesnew2.sh and replace "avconv" with ffmpeg
module load ffmpeg/2.1.4
sh makelinkimagenew2.sh # general updated way of making the final mp4 files.
# scp movies to your computer and post to youtube for me.

13) In general, the python produces final output that you want to copy to your computer or some computer.  The stuff you want is:  *.png *.eps *.pdf data*.txt *.mp4  .  You can use scp to copy it somewhere.

14) I checked all these steps with a new 128x64x1 run with 43 files created after 1 hour run.  All these steps work as a suggested with the latest python stuff.  Of course, as you go to larger simulations and more fieldline files, you'll get the hang of how long things take and how to tune things like "timetot".  At the moment, for my 128x64x1 run with 43 files, each step takes only a few minutes.  So easy to check and understand at only 50 files and only move on once things work.








=====
Running many restarted jobs.
=====

In general, I recommend restarting manually for a while.  Only do up to step #4 below and then manually sbatch each "b" or more *after* each previous job is done.

1) Go into your code directory on stampede.  This is where the batch file is.  Ensure the name of the batch file looks like: batch.slurm.kraken.<runname>a where <runname> is replaced by (e.g.) rad0.3 .  Ensure it's setup as a firstrun=1 and the "sh stampederestartsustain_rad1.sh 0 0 4 radtma0.8 radtma0.8 a b" type line really has "a b" there.

2) cp batches/makebatches.sh .  ; cp batches/stampederestartsustain_rad1.sh .

3) Edit makebatches.sh and replace "radtam0.8" with your <runname> .

4) sh makebatches.sh  .  This will create a bunch of batch files that can be used for restarting the job.  Look at a random batch file letter (e.g. "p") and ensure it makes sense.


For automated restarts, do #5+ below  (only do if Jon approves)...


5) cp batches/submitdeplist.sh .

6) Edit submitdeplist.sh and again change "radtma0.8" to your <runname> .

As written, the assumption is that you ran "radtam0.8a" manually by calling sbatch batch.slurm.kraken.radtma0.8a .  Then, once that completed and you checked the data and want to restart, only then after such checks do you think about calling submitdeplist.sh.

However, there are by default *WAY* too many restarts setup here.  Better is to take things a step at a time.  Try removing all latter lines and only keep up through the "d" submission to sbatch.

Then, there is the question about whether you ran this submitdeplist.sh *while* the batch.slurm.kraken.radtma0.8a was going *or* if you ran after that job was completed.  By default, submitdeplist.sh is setup as if you ran the "a" job with sbatch manually, and then immediately started this submitdeplist.sh .  DO NOT do this until you are more experienced.

Instead, again, check the data by using python on the "a" data.  Then once you want to restart non-manually (if you want to), then further edit submitdeplist.sh so the first JOBID=4915427 is commented out and you replace:

JOBID=`sbatch --dependency=afterany:$JOBID batch.slurm.kraken.radtma0.8b | tail -1 | awk '{print $4}'`

with

JOBID=`sbatch batch.slurm.kraken.radtma0.8b | tail -1 | awk '{print $4}'`

This just removes the dependence on "a" because here we assume "a" was already completed.

7) Now that submitdeplist.sh is designed appropriately, you can run:

sh submitdeplist.sh

This will submit a dependent list of jobs so when one job is done, the next starts automatically.

8) In general, you have be more knowledgeable to handle submitdeplist.sh .  You have to ensure the jobs don't overload the system, and you need to be able to kill jobs in case something goes wrong.  This means killing the entire set of queued jobs and knowing how to edit submitdeplist.sh to continue appropriately by deleting any older undesired lettered jobs and making the letter you want to start with have no dependency, and then having the latter letters have dependency.


======




